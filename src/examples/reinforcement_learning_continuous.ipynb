{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0649b76a",
   "metadata": {},
   "source": [
    "# Init Bionic VTOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96751412",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../Flyonic.jl\");\n",
    "using .Flyonic; #simulator kram\n",
    "\n",
    "using Rotations; # used for initial position\n",
    "\n",
    "#find packages on julia hub\n",
    "using ReinforcementLearning; \n",
    "using StableRNGs;\n",
    "using Flux;\n",
    "using Flux.Losses;\n",
    "using Random;\n",
    "using IntervalSets;\n",
    "using LinearAlgebra;\n",
    "using Distributions;\n",
    "\n",
    "using Plots;\n",
    "using Statistics;\n",
    "\n",
    "using BSON: @save, @load # save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_visualization(); #from flyonics package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9557df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicates how many threads Julia was started with. This is important for the multi-threaded environment\n",
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411db62",
   "metadata": {},
   "source": [
    "# Create Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect RL-library with simulation via this environment\n",
    "mutable struct VtolEnv{A,T,ACT,R<:AbstractRNG} <: AbstractEnv # Parametric Constructor for a subtype of AbstractEnv\n",
    "    #required\n",
    "    action_space::A\n",
    "    observation_space::Space{Vector{ClosedInterval{T}}}\n",
    "    state::Vector{T} #state of system; goes in policy\n",
    "    action::ACT #action that system does next\n",
    "    done::Bool #e.g. drone crashed\n",
    "    t::T\n",
    "    rng::R\n",
    "\n",
    "    name::String #for multible environoments\n",
    "    visualization::Bool\n",
    "    realtime::Bool # For humans recognizable visualization\n",
    "    \n",
    "    # Everything you need aditionaly can also go in here.\n",
    "    #additional states for simulation; not for policy\n",
    "    x_W::Vector{T}\n",
    "    v_B::Vector{T}\n",
    "    a_B::Vector{T}\n",
    "    R_W::Matrix{T}\n",
    "    ω_B::Vector{T}\n",
    "    α_B::Vector{T}\n",
    "    \n",
    "    wind_W::Vector{T}\n",
    "    \n",
    "    Δt::T\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a keyword-based constructor for the type declared in the mutable struct typedef. \n",
    "# It could also be done with the macro Base.@kwdef.\n",
    "function VtolEnv(;\n",
    "     \n",
    "    #continuous = true,\n",
    "    rng = Random.GLOBAL_RNG, # Random number generation\n",
    "    name = \"vtol\",\n",
    "    visualization = false,\n",
    "    realtime = false,\n",
    "    kwargs... # let the function take an arbitrary number of keyword arguments \n",
    ")\n",
    "    \n",
    "    T = Float64; # explicit type which is used e.g. in state. Cannot be altered due to the poor matrix defininon.\n",
    "\n",
    "    #action_space = Base.OneTo(21) # 21 discrete positions for the flaps\n",
    "    \n",
    "    #here: two actions; continuous\n",
    "    #Beginning: stay 2D\n",
    "    #Later: 4D: independent rotors and flaps (thats all actuators)\n",
    "    action_space = Space(\n",
    "        ClosedInterval{T}[\n",
    "            0.0..2.0, # thrust in N\n",
    "            -1.0..1.0, # flaps in rad\n",
    "            ], \n",
    "    )\n",
    "\n",
    "    #reduced to 2D for now\n",
    "    state_space = Space( # Three continuous values in state space.\n",
    "        ClosedInterval{T}[\n",
    "            typemin(T)..typemax(T), # rotation arround y\n",
    "            typemin(T)..typemax(T), # rotation velocity arround y\n",
    "            typemin(T)..typemax(T), # world position along x\n",
    "            typemin(T)..typemax(T), # world position along z\n",
    "            ], \n",
    "    )\n",
    "    \n",
    "    if visualization\n",
    "        create_VTOL(name, actuators = true, color_vec=[1.0; 1.0; 0.6; 1.0]);\n",
    "        set_transform(name, [0.0; 0.0; 0.0] ,QuatRotation([1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0]));\n",
    "        set_actuators(name, [0.0; 0.0; 0.0; 0.0])\n",
    "    end\n",
    "\n",
    "    #instantiates the sctruct from before\n",
    "    environment = VtolEnv(\n",
    "        action_space,\n",
    "        state_space,\n",
    "        zeros(T, 4), # current state, needs to be extended.\n",
    "        rand(action_space),\n",
    "        false, # episode done ?\n",
    "        0.0, # time\n",
    "        rng, # random number generator  \n",
    "        name,\n",
    "        visualization, # visualization\n",
    "        realtime, # realtime visualization\n",
    "        zeros(T, 3), # x_W\n",
    "        zeros(T, 3), # v_B\n",
    "        zeros(T, 3), # a_B\n",
    "        Array{T}([1 0 0; 0 1 0; 0 0 1]),\n",
    "        zeros(T, 3), # ω_B\n",
    "        zeros(T, 3), # α_B\n",
    "        zeros(T, 3), # wind_W\n",
    "        T(0.025), # Δt  \n",
    "    )\n",
    "    \n",
    "    #do this for simulation start\n",
    "    reset!(environment)\n",
    "    \n",
    "    return environment\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec660d5e",
   "metadata": {},
   "source": [
    "Just for explanation:\n",
    "\n",
    "1. A mutable Struct is created. A struct is a constructor and a constructor is a function that creates new objects.\n",
    "2. A outer keyword-based constructor method is added for the type declared in the mutable struct typedef before.\n",
    "\n",
    "So now we have a function with two methods. Julia will decide which method to call by multiple dispatch."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc8a034d",
   "metadata": {},
   "source": [
    "methods(VtolEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806413d1",
   "metadata": {},
   "source": [
    "# Define the RL interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f822029",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(env::VtolEnv, seed) = Random.seed!(env.rng, seed)\n",
    "RLBase.action_space(env::VtolEnv) = env.action_space\n",
    "RLBase.state_space(env::VtolEnv) = env.observation_space\n",
    "RLBase.is_terminated(env::VtolEnv) = env.done\n",
    "RLBase.state(env::VtolEnv) = env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fb89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function computeReward(env::VtolEnv{A,T}) where {A,T}\n",
    "    \n",
    "    #this rewards makes drone go straight up :)\n",
    "    stay_alive = 3.0\n",
    "    not_upright_orientation = abs(env.state[1]-pi*0.5)*10.0\n",
    "    not_centered_position = abs(env.state[3])*10.0\n",
    "    hight = env.state[4]*100.0\n",
    "    \n",
    "    return stay_alive - not_upright_orientation - not_centered_position + hight\n",
    "end\n",
    "\n",
    "\n",
    "RLBase.reward(env::VtolEnv{A,T}) where {A,T} = computeReward(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "function RLBase.reset!(env::VtolEnv{A,T}) where {A,T}\n",
    "    \n",
    "    # Visualize initial state\n",
    "    set_transform(env.name, env.x_W,QuatRotation(env.R_W));\n",
    "    set_actuators(env.name, [0.0; 0.0; 0.0; 0.0])\n",
    "    \n",
    "    env.x_W = [0.0; 0.0; 0.0];\n",
    "    env.v_B = [0.0; 0.0; 0.0];\n",
    "    env.a_B = [0.0; 0.0; 0.0];\n",
    "    env.R_W = Matrix(UnitQuaternion(RotY(-pi/2.0)*RotX(pi)));\n",
    "    env.ω_B = [0.0; 0.0; 0.0];\n",
    "    env.α_B = [0.0; 0.0; 0.0];\n",
    "    \n",
    "    env.wind_W = [0.0; 0.0; 0.0];\n",
    "    \n",
    " \n",
    "    env.state = [env.ω_B[2]; Rotations.params(RotYXZ(env.R_W))[1]; env.x_W[1]; env.x_W[3]]\n",
    "    \n",
    "    env.t = 0.0\n",
    "    env.action = [0.0]\n",
    "    env.done = false\n",
    "    nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a methods for a callable object.\n",
    "# So when a VtolEnv object is created, it has this method that can be called\n",
    "# Apply chosen actions on simulator\n",
    "# add a third function to environment with action a\n",
    "function (env::VtolEnv)(a)\n",
    "\n",
    "    # set the propeller trust and the two flaps 2D case\n",
    "    next_action = [a[1], a[1], a[2], a[2]]\n",
    "   \n",
    "    _step!(env, next_action)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VtolEnv();"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c6c7550",
   "metadata": {},
   "source": [
    "methods(env) # Just to explain which methods the object has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual step for each simulation iteration\n",
    "# Trajectory: generate externally; and then do some calculation in the step; in the environment add additional state: v_soll; v_ist --> use in reward function\n",
    "# Generate trajectories randomly, so that iterations train on different trajectories\n",
    "# Test set: always use same trajectory\n",
    "# TODO maybe we can get trajectory generation from last year\n",
    "# Evaluation metric: how well do I track my velocity trajectory? Look for 2-3 metrics here and thats ok\n",
    "function _step!(env::VtolEnv, next_action)\n",
    "        \n",
    "    # caluclate wind impact\n",
    "    v_in_wind_B = vtol_add_wind(env.v_B, env.R_W, env.wind_W)\n",
    "    # caluclate aerodynamic forces\n",
    "    torque_B, force_B = vtol_model(v_in_wind_B, next_action, eth_vtol_param);\n",
    "    # integrate rigid body dynamics for Δt\n",
    "    env.x_W, env.v_B, env.a_B, env.R_W, env.ω_B, env.α_B, time = rigid_body_simple(torque_B, force_B, env.x_W, env.v_B, env.R_W, env.ω_B, env.t, env.Δt, eth_vtol_param)\n",
    "\n",
    "\n",
    "    if env.realtime\n",
    "        sleep(env.Δt) # TODO: just a dirty hack. this is of course slower than real time.\n",
    "    end\n",
    "    \n",
    "    # Visualize the new state\n",
    "    if env.visualization\n",
    "        # TODO: Can be removed for real trainings\n",
    "        set_transform(env.name, env.x_W, QuatRotation(env.R_W));\n",
    "        set_actuators(env.name, next_action)\n",
    "    end\n",
    " \n",
    "    env.t += env.Δt\n",
    "    \n",
    "    # State space\n",
    "    # Pass more states to RF-Learning-Agent, if required\n",
    "    rot = Rotations.params(RotYXZ(env.R_W))[1]\n",
    "    env.state[1] = rot # rotation arround y\n",
    "    env.state[2] = env.ω_B[2] # rotation velocity arround y\n",
    "    env.state[3] = env.x_W[1] # world position along x\n",
    "    env.state[4] = env.x_W[3] # world position along z\n",
    "    \n",
    "    \n",
    "    # Termination criteria\n",
    "    env.done =\n",
    "        #norm(v_B) > 2.0 || # stop if body is too fast\n",
    "        env.x_W[3] < -1.0 || # stop if body is below -1m\n",
    "        0.0 > rot || # Stop if the drone is pitched 90°.\n",
    "        rot > pi || # Stop if the drone is pitched 90°.\n",
    "        env.t > 10 # stop after 10s\n",
    "    nothing #return nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "RLBase.test_runnable!(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223a31f",
   "metadata": {},
   "source": [
    "Show an overview of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6de74",
   "metadata": {},
   "source": [
    "# Setup of a reinforcement learning experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5683fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123    \n",
    "rng = StableRNG(seed)\n",
    "    N_ENV = 8 #number of envs\n",
    "    UPDATE_FREQ = 1024\n",
    "    \n",
    "    \n",
    "    # define multiple environments for parallel training\n",
    "    env = MultiThreadEnv([\n",
    "        # use different names for the visualization\n",
    "        VtolEnv(; rng = StableRNG(hash(seed+i)), name = \"vtol$i\") for i in 1:N_ENV\n",
    "    ]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f128b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function approximator\n",
    "    ns, na = length(state(env[1])), length(action_space(env[1]))\n",
    "    #ActorCritic Policy\n",
    "    approximator = ActorCritic(\n",
    "                #ns - number states as input\n",
    "                #3 layer; last layer splitted in mean and variance; then action is sampled\n",
    "                actor = GaussianNetwork(\n",
    "                    pre = Chain(\n",
    "                    Dense(ns, 16, relu; initW = glorot_uniform(rng)),#\n",
    "                    Dense(16, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    ),\n",
    "                    μ = Chain(Dense(16, na; initW = glorot_uniform(rng))),\n",
    "                    logσ = Chain(Dense(16, na; initW = glorot_uniform(rng))),\n",
    "                ),\n",
    "                critic = Chain(\n",
    "                    Dense(ns, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    Dense(16, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    Dense(16, 1; initW = glorot_uniform(rng)),\n",
    "                ),\n",
    "                optimizer = ADAM(1e-3),\n",
    "            );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea4c37c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    #learning\n",
    "    agent = Agent( # A wrapper of an AbstractPolicy\n",
    "        # AbstractPolicy: the policy to use\n",
    "        policy = PPOPolicy(;\n",
    "                    approximator = approximator |> gpu,\n",
    "                    update_freq=UPDATE_FREQ,\n",
    "                    dist = Normal,\n",
    "                    # For parameters visit the docu: https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.PPOPolicy\n",
    "                    ),\n",
    "        \n",
    "        # AbstractTrajectory: used to store transitions between an agent and an environment source\n",
    "        # depends on RL-Algorithm\n",
    "        trajectory = PPOTrajectory(;\n",
    "            capacity = UPDATE_FREQ,\n",
    "            state = Matrix{Float64} => (ns, N_ENV),\n",
    "            action = Matrix{Float64} => (na, N_ENV),\n",
    "            action_log_prob = Vector{Float64} => (N_ENV,),\n",
    "            reward = Vector{Float64} => (N_ENV,),\n",
    "            terminal = Vector{Bool} => (N_ENV,),\n",
    "        ),\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f158a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "function saveModel(t, agent, env)\n",
    "    model = cpu(agent.policy.approximator)   \n",
    "    f = joinpath(\"./RL_models/\", \"vtol_ppo_$t.bson\")\n",
    "    @save f model\n",
    "    println(\"parameters at step $t saved to $f\")\n",
    "end"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29e49f95",
   "metadata": {},
   "source": [
    "function loadModel()\n",
    "    # TODO use correct relative path here\n",
    "    f = joinpath(\"./RL_models/\", \"vtol_ppo_1000000.bson\")\n",
    "    @load f model\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bcc9142",
   "metadata": {},
   "source": [
    "#use pretrained model\n",
    "agent.policy.approximator = loadModel();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e05772",
   "metadata": {},
   "outputs": [],
   "source": [
    "function validate_policy(t, agent, env)\n",
    "    run(agent.policy, test_env, StopAfterEpisode(1), episode_test_reward_hook)\n",
    "    # the result of the hook\n",
    "    println(\"test reward at step $t: $(mean(episode_test_reward_hook.rewards))\")\n",
    "    \n",
    "end;\n",
    "\n",
    "episode_test_reward_hook = TotalRewardPerEpisode(;is_display_on_exit=false)\n",
    "# create a env only for reward test\n",
    "test_env = VtolEnv(;name = \"testVTOL\", visualization = true, realtime = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb737010",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReinforcementLearning.run(\n",
    "    agent,\n",
    "    env,\n",
    "    StopAfterStep(1_000_000),\n",
    "    ComposedHook(\n",
    "        DoEveryNStep(saveModel, n=100_000), \n",
    "        DoEveryNStep(validate_policy, n=10_000)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbfea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(episode_test_reward_hook.rewards)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "258f946b",
   "metadata": {},
   "source": [
    "close_visualization(); # closes the MeshCat visualization"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 12 Threads 1.8.0",
   "language": "julia",
   "name": "julia-12-threads-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
